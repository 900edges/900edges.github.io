<!DOCTYPE html>

<html>
  
<head>
<meta charset = "UTF=8">
<meta name="viewport" content = "width = device-width, initial-scale = 1.0">
<title>Title</title>


<body>
  <body style = "background-color:#F5C366;">
    <h1 style = "text-align:center;"> Math</h1>
    
    
    <h2> Research </h2> 
    <p>
      I'm working in the intersection of large-graph limits, variational calculus, and nonlocal analysis. 
      My current research involves graph Ginzburg-Landau theory and nonlocal perimeter. 
      This research has applications in graph cut problems, image segmentation and social network analysis, among other questions. 
      This research is in collaboration with <a href="https://sites.google.com/view/jamesmichaelscott/home">James Scott</a>, 
      <a href="http://www.columbia.edu/~qd2125/">Qiang Du</a>, and <a href="https://www.math.ucla.edu/~mason/">Mason Porter</a>. 
    </p>
    <p>Graphons are a notion of large-graph limit (and the notion of graph-to-graphon convergence 
        is defined with respect to a norm called "cut norm"). 
    </p>
    
    <p>
      The next piece of this project is to consider dynamics on graphons, in particular the graphon heat equation as studied by 
      <a href="https://arxiv.org/pdf/1302.5804.pdf">Medvedev</a> and the graphon Allen--Cahn equation as an extension of the work by 
      <a href="https://apps.dtic.mil/sti/tr/pdf/AD1014925.pdf>Luo and Bertozzi</a>.  
    </p>
    <p>
      I'm also working on providing theoretical guaranteees for variational inference, a popular statistical inference algorithm. 
      This research is in progress with <a href="https://kw2934.github.io/">Kaizheng Wang</a>. 
    </p>
    <p>
      If you're interested in collaborating or learning more about these research projects, 
      please <a href = "mailto: ejz2120@columbia.edu">email me!</a>
    </p>
    
    
    <h2> CV </h2>
    <a href="academic_cv (1).pdf">CV as of fall 2024</a>
    
    <br>
    
    
    
    <h2> Publications </h2>
    
    <a href="https://scholar.google.com/citations?user=4fn8bpgAAAAJ&hl=en&oi=ao"> Google scholar </a>
    <br>
    <br>
    <a href="https://arxiv.org/abs/2408.00422">Ginzburg--Landau functionals in the large-graph limit (preprint, 2024)</a>
    <ul>
      <li> Ginzburg--Landau (GL) theory, which was originally developed to model superconductors, has more recently been applied to 
        a variety of applications including graph clustering, image processing, and min-cut/max-flow problems. These applications typically
        use the graph GL functional, which is an approximation of the graph-cut functional. The GL functional is the sum of a Dirichlet energy 
        term, which penalizes the variability of a function, and a double-well potential, which penalizes deviations from the values +1 and -1. 
        
        This paper defines a graphon GL functional and shows that the minimizers of the graph GL functional converge to the 
        minimizers of the graphon GL functional. 
      </li>
    </ul>
    <br>
    <a href="https://arxiv.org/pdf/2210.11385.pdf">On Representations of Mean-Field Variational Inference (preprint, 2022)</a>
    <ul>
      <li> Variational inference is an approach to Bayesian inference that is based on an optimization problem. 
        It is widely-used, but VI algorithms have lacked theoretical guarantees. The idea of VI is to seek a 
        constrained minimizer of the distance to the Bayesian posterior, 
        where the constrained set is the set of product measures (probability measures where each component is independent of all others). 
        Minimizations in probability space can in principle be approximated by a gradient flow with respect to a Wasserstein metric; in the case 
        of the unconstrained Bayesian problem, the Wasserstein--gradient flow is a Fokker--Planck equation. 
        The problem we tackle is: how do we describe the constrained problem as a gradient flow in constrained space? 
        We express variational inference as an approximate Wasserstein gradient flow, and we discuss theoretical implications. </li>
    </ul>
    
    <a href="http://bayesiandeeplearning.org/2021/papers/3.pdf"> Unveiling Mode-Connectivity of the ELBO Landscape (Workshop paper, 2021)</a> 
    <ul>
      <li> Mode-connectivity is an interesting property that has been discovered in neural net loss functions, which are high-dimensional and nonconvex. 
      It has been related to a "no bad local minima" property that's been noticed in practice, and is related to overparametrization. 
      This workshop paper demonstrates and begins to explain the existence of mode-connectivity in the ELBO, which is the loss function of 
        variational inference. This paper began as my final project for a course (and it was nominated by the professor in a "Best Student Project" competition!).
        The workshop paper is an abbreviated version of my final project. 
    </ul>

    <h2> In-Progress Publications </h2>
    Graphon Reaction--Diffusion Equations
      <li> 
        Reaction--Diffusion equations are ... 
      </ul>
    A Particle Algorithm for Mean-Field Variational Inference
      
    <br>
    
    
    
    
    
    <h2>Teaching</h2>
    Adjunct Professor: Calculus II at <a href="https://cooper.edu/">The Cooper Union for the Advancement of Science and Art</a> (Spring 2025) 
    <br>
    Grader: Mathematics for data science (Spring 2020), Linear algebra (Fall 2021), Intro to numerical methods (Spring 2022), Linear algebra (Fall 2022), Numerical analysis (Fall 2023)
    <br> 
    TA: Partial Differential Equations (Fall 2019)
    <br>
    Math tutor: UVA Athletics, UVA Mathematics (2016-2019)
    
    
    <br>
    <h2>Seminar</h2>
    <a href="http://www.columbia.edu/~nhw2114/am_seminar_fall23/" > Applied Math student seminar </a>
    <br>
    This semester (Spring 2024), the Applied Math student seminar meets on Thursdays, 1-2pm in the APAM conference room. 
    <br>
    I co-founded the AM seminar in spring 2023 to gather with our 
    fellow graduate students to present recent learnings and progress, practice giving talks and feedback on talks. 
    
  

    
    
</body>
</html>
