<!DOCTYPE html>

<html>
  
<head>
<meta charset = "UTF=8">
<meta name="viewport" content = "width = device-width, initial-scale = 1.0">
<title>Title</title>


<body>
  <body style = "background-color:#F5C366;">
    <h1 style = "text-align:center;"> Math</h1>

    <p> I'm a 6th year PhD student in applied math at Columbia university. My research relates to large-graph limits of 
      graph optimization problems, reaction--diffusion equations on graphs, and stochastic processes on graphs.
      My past work on variational inference relates to proving theoretical guarantees for mean-field variational inference,
      studying Wasserstein gradient flows, Fokker-Planck equations and their convergence properties. 
      <br>
      <br>
      I graduate in May 2025, and I am currently applying to postdoc positions. I am primarily interested in network science, calculus of variations, 
      stochastic processes, and interacting particle systems. Please see my <a href="https://edithjinzhang.github.io/math.html">math page</a>. 
      
    </p>
    
    <h2> Research </h2> 
    <p>
      My current research relates to <a href="https://www.ams.org/notices/201501/rnoti-p46.pdf">graphons</a>, which are 
      a notion of infinite graphs. Because they are continuous objects, graphons are well-suited to framing graph problems
      with functional analysis, and in particular, non-local analysis. For example, my paper on graphon Ginzburg--Landau functionals
      uses Young measures and Gamma-convergence, which is a type of convergence that is used in the calculus of variations. 
      My (forthcoming) paper on graphon reaction--diffusion equations uses techniques from numerical analysis and stochastic analysis. 
    
    </p>
  
    
    <h2> CV </h2>
    <a href="cv.pdf">CV (updated Dec 2024)</a>
    
    <br>
    
    
    
    <h2> Publications </h2>
    
    <a href="https://scholar.google.com/citations?user=4fn8bpgAAAAJ&hl=en&oi=ao"> Google scholar </a>
    <br>
    <br>
    <a href="https://arxiv.org/abs/2408.00422">Ginzburg--Landau functionals in the large-graph limit (preprint, 2024)</a>
    <ul>
      <li> Ginzburg--Landau (GL) theory, which was originally developed to model superconductors, has more recently been applied to 
        a variety of applications including graph clustering, image processing, and min-cut/max-flow problems. These applications typically
        use the graph GL functional, which is an approximation of the graph-cut functional. The GL functional is the sum of a Dirichlet energy 
        term, which penalizes the variability of a function, and a double-well potential, which penalizes deviations from the values +1 and -1. 
        
        This paper defines a graphon GL functional and shows that the minimizers of the graph GL functional converge to the 
        minimizers of the graphon GL functional. 
      </li>
    </ul>
    <br>
    <a href="https://arxiv.org/pdf/2210.11385.pdf">On Representations of Mean-Field Variational Inference (preprint, 2022)</a>
    <ul>
      <li> Variational inference is an approach to Bayesian inference that is based on an optimization problem. 
        It is widely-used, but VI algorithms have lacked theoretical guarantees. The idea of VI is to seek a 
        constrained minimizer of the distance to the Bayesian posterior, 
        where the constrained set is the set of product measures (probability measures where each component is independent of all others). 
        Minimizations in probability space can in principle be approximated by a gradient flow with respect to a Wasserstein metric; in the case 
        of the unconstrained Bayesian problem, the Wasserstein--gradient flow is a Fokker--Planck equation. 
        The problem we tackle is: how do we describe the constrained problem as a gradient flow in constrained space? 
        We express variational inference as an approximate Wasserstein gradient flow, and we discuss theoretical implications. </li>
    </ul>
    
    <a href="http://bayesiandeeplearning.org/2021/papers/3.pdf"> Unveiling Mode-Connectivity of the ELBO Landscape (Workshop paper, 2021)</a> 
    <ul>
      <li> Mode-connectivity is an interesting property that has been discovered in neural net loss functions, which are high-dimensional and nonconvex. 
      It has been related to a "no bad local minima" property that's been noticed in practice, and is related to overparametrization. 
      This workshop paper demonstrates and begins to explain the existence of mode-connectivity in the ELBO, which is the loss function of 
        variational inference. This paper began as my final project for a course (and it was nominated by the professor in a "Best Student Project" competition!).
        The workshop paper is an abbreviated version of my final project. 
    </ul>

    <h2> In-Progress Publications </h2>
    Graphon Reaction--Diffusion Equations
      <li> 
        A graph reaction--diffusion (RD) equation is a system of differential equations that is defined on the nodes of a graph. 
        Consider a sequence of growing graphs that converges to a limiting graphon. 
        In this paper, we show that the sequence of solutions of the sequence of graph RD equations converges to the solution of a limiting 
        non-local RD equation, which we call a graphon RD equation. Furthermore, we show that a sequence of stochastic particle 
        processes (that consist of a random walk and a birth--death process) on the sequence of graphs converges to the solution
        of the graphon RD equation. This project is joint with <a href="https://www.apam.columbia.edu/faculty/qiang-du"> Qiang Du </a>
        and <a href="https://sites.google.com/view/jamesmichaelscott/home">James Scott</a>. 
    ,
      </ul>
    <br>
    A Particle Algorithm for Mean-Field Variational Inference
      <li> 
        As stated above (in the description of "On Representations of Mean-Field Variational Inference"), variational inference (VI) is 
        a Bayesian statistical inference method that relies on a constrained optimization problem. In this paper, we propose a particle-based
        algorithm that efficiently approximates variational inference. We describe convergence guarantees. 
        This research is in progress with <a href="https://kw2934.github.io/">Kaizheng Wang</a>
      </ul>
    <br>
    
    
    
    
    
    <h2>Teaching</h2>
    Adjunct Professor: Calculus II at <a href="https://cooper.edu/">The Cooper Union for the Advancement of Science and Art</a> (Spring 2025) 
    <br>
    Grader: Mathematics for data science (Spring 2020), Linear algebra (Fall 2021), Intro to numerical methods (Spring 2022), Linear algebra (Fall 2022), Numerical analysis (Fall 2023)
    <br> 
    TA: Partial Differential Equations (Fall 2019)
    <br>
    Math tutor: UVA Athletics, UVA Mathematics (2016-2019)
    
    
    <br>
    <h2>Seminar</h2>
    <a href="http://www.columbia.edu/~nhw2114/am_seminar_fall23/" > Applied Math student seminar </a>
    <br>
    This semester (Spring 2024), the Applied Math student seminar meets on Thursdays, 1-2pm in the APAM conference room. 
    <br>
    I co-founded the AM seminar in spring 2023 to gather with our 
    fellow graduate students to present recent learnings and progress, practice giving talks and feedback on talks. 
    
  

    
    
</body>
</html>
